{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ba39cf-21de-4e9d-bee5-9b7cac490b23",
   "metadata": {},
   "source": [
    "# Prereq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c172bca0-941d-451d-b4a7-033a0a68ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR, StepLR\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd98c0b-b980-4907-8215-b7e7dba483dc",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3fc3da-62b2-4820-965e-5c1b44518a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Data(Dataset):\n",
    "    def __init__(self, data_files, label_files, transform=None):\n",
    "        self.data_matrices = np.load(data_files+'/y.npy')\n",
    "        self.label_matrices = np.load(label_files+'/x.npy')\n",
    "        self.transform = transform\n",
    "\n",
    "        for i in range(len(self.label_matrices)):\n",
    "            self.label_matrices[i][self.label_matrices[i] == 0] = 100  #Add 100 ohm resistance as open circuit\n",
    "        # Ensure the number of data and label matrices match\n",
    "        assert len(self.data_matrices) == len(self.label_matrices), \"Number of data and label files must match\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_matrices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_matrices[idx]\n",
    "        label = self.label_matrices[idx]\n",
    "\n",
    "        # print(np.where(label==100,0,1),'\\n',label)\n",
    "\n",
    "        sample = {'features': torch.unsqueeze(torch.tensor(data, dtype=torch.float32),0) / 5.0,\n",
    "                  'target': torch.tensor(label, dtype=torch.float32) / 100.0,\n",
    "                 'label': torch.tensor(np.where(label == 100.0, 0, 1), dtype=torch.float32)\n",
    "                 }\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c66ede-8ea5-4422-99cd-80cd46935e09",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d687cd-718d-4d06-9739-2aff798295af",
   "metadata": {},
   "source": [
    "## Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4170936c-4c8b-4d19-bb16-e7a54da13cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_channels, height, width):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.positional_encoding = self.create_positional_encoding()\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        pe = torch.zeros(self.num_channels, self.height, self.width)\n",
    "        # Use a large enough max length for sin/cos calculation\n",
    "        # max_len = max(self.height, self.width)\n",
    "        for pos in range(self.width):\n",
    "            for i in range(0, self.num_channels, 2):\n",
    "                if i < self.num_channels:\n",
    "                    pe[i, :, pos] = math.sin(pos / (10 ** (i / self.num_channels)))\n",
    "                    if i + 1 < self.num_channels:\n",
    "                        pe[i + 1, :, pos] = math.cos(pos / (10 ** (i / self.num_channels)))\n",
    "                        \n",
    "        for pos in range(self.height):\n",
    "            for i in range(0, self.num_channels, 2):\n",
    "                if i < self.num_channels:\n",
    "                    pe[i, pos, : ] += math.sin(pos / (100 ** (i / self.num_channels)))\n",
    "                    if i + 1 < self.num_channels:\n",
    "                        pe[i + 1,pos,: ] += math.cos(pos / (100 ** (i / self.num_channels)))\n",
    "        return pe.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding.to(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80461796-52c2-4dff-b9f7-289dd4b69731",
   "metadata": {},
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d64ce09-8879-4751-bd48-b405da3e0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,num_iteration):\n",
    "        super().__init__()\n",
    "        self.num_iteration = num_iteration\n",
    "        self.dropout_prob = 0.4\n",
    "        self.Conv1=nn.Conv2d(25,25,(1,11))\n",
    "        self.Conv2= nn.Conv2d(25,25,(7,1))\n",
    "        self.Conv3= nn.Conv2d(25,2,(1,1))\n",
    "        self.Conv_list1 = nn.ModuleList([nn.Conv2d(25, 25, (1, 11)) for _ in range(num_iteration)])\n",
    "        self.Conv_list2 = nn.ModuleList([nn.Conv2d(25, 25, (7, 1)) for _ in range(num_iteration)]) \n",
    "        \n",
    "        self.Tconv1 = nn.ConvTranspose2d(25,25,(1,11))\n",
    "        self.Tconv2 = nn.ConvTranspose2d(25,25,(7,1))\n",
    "        \n",
    "            \n",
    "        self.Tconv_list1 = nn.ModuleList([nn.ConvTranspose2d(25,25,(1,11)) for _ in range(num_iteration)])\n",
    "        self.Tconv_list2 = nn.ModuleList([nn.ConvTranspose2d(25,25,(7,1)) for _ in range(num_iteration)])\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(25, 7, 11)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(p=0.4)\n",
    "    def forward(self, x):\n",
    "        x=x.repeat(1, 25, 1, 1)\n",
    "        pos_emb = self.positional_encoding(x)\n",
    "\n",
    "        pos_emb = 0.2*(pos_emb/2)\n",
    "        x=torch.add(pos_emb,x)\n",
    "        \n",
    "        orgnl=x\n",
    "        z=x\n",
    "        for i in range(self.num_iteration):\n",
    "            conv_layer1 = self.Conv_list1[i]\n",
    "            conv_layer2 = self.Conv_list2[i]\n",
    "            tconv_layer1 = self.Tconv_list1[i]\n",
    "            tconv_layer2 = self.Tconv_list2[i]\n",
    "            \n",
    "            a=F.relu(self.Conv1(x))\n",
    "            a=self.Tconv1(a)\n",
    "            b=F.relu(self.Conv2(x))\n",
    "            b=self.Tconv2(b)\n",
    "            x_middle=torch.sum(torch.stack((a,b),dim=1),1)\n",
    "            x_middle=self.dropout1(x_middle)\n",
    "            \n",
    "            a = F.relu(conv_layer1(x_middle))\n",
    "            a = tconv_layer1(a)\n",
    "            b = F.relu(conv_layer2(x_middle))  \n",
    "            b = tconv_layer2(b)\n",
    "            x = torch.sum(torch.stack((a,b),dim=1),1)\n",
    "            x = self.dropout2(x)\n",
    "            x1 = torch.add(z,x)\n",
    "            x = F.relu(x1)  \n",
    "            z=x\n",
    "        x = self.Conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41ea3d-db43-44c9-a897-bec863a5b251",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58dec867-f74c-4e8f-8d06-cf58d4365d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def Training(data_path,label_path,batch,scdulr='',lr=1e-3,depth=10,epoch=100):\n",
    "\n",
    "    dataset = Load_Data(data_path, label_path)\n",
    "    num_epochs = epoch\n",
    "\n",
    "    a=np.shape(dataset)\n",
    "\n",
    "    \n",
    "    seed = 27 #previous12,27\n",
    "    # random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    train_ratio = 0.6\n",
    "    val_ratio = 0.1\n",
    "    test_ratio = 0.1\n",
    "\n",
    "    unseen_ratio = 0.2\n",
    "    train_ratio =0.8\n",
    "\n",
    "\n",
    "    batch_size=batch\n",
    "    \n",
    "    val_error=[]\n",
    "    tst_error=[]\n",
    "    f1_list=[]\n",
    "    res_error=[]\n",
    "\n",
    "    \n",
    "    training_size=int(train_ratio*a[0])\n",
    "\n",
    "    training_dataset = Subset(dataset, range(training_size))\n",
    "    \n",
    "    unseen_dataset = torch.utils.data.Subset(dataset, range(training_size, a[0]))\n",
    "\n",
    "    separate_dataloader = DataLoader(unseen_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    net = Net(num_iteration = depth)\n",
    "    net = net.double()\n",
    "    \n",
    "    net = net.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    label_criterion=nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-3)\n",
    "    # optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    if scdulr=='Platue':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=2)\n",
    "    if scdulr=='Cosine':\n",
    "        scheduler = CosineAnnealingLR(optimizer, num_epochs, eta_min=lr * 1e-6)\n",
    "\n",
    "    train_size = int(train_ratio * len(training_dataset))\n",
    "    val_size = int(val_ratio * len(training_dataset))\n",
    "    test_size = len(training_dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(training_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    alpha,beta=0.91,0.09\n",
    "\n",
    "    epsilon=1e-8\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        if epoch>0:\n",
    "            alpha = 1/(output_loss.item() +epsilon)\n",
    "            beta = 1/(label_loss.item() + epsilon)\n",
    "            weight_sum = alpha + beta\n",
    "            alpha /= weight_sum\n",
    "            beta /= weight_sum\n",
    "\n",
    "        # if((epoch+1)%15==0):\n",
    "        #     if alpha<0.89:\n",
    "        #         alpha,beta=0.91,0.09\n",
    "        for batch in train_dataloader:\n",
    "            features, targets, labels = batch['features'], batch['target'], batch['label']\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            features = features.squeeze(1)\n",
    "            targets = targets.squeeze(1)\n",
    "            labels = labels.squeeze(1)\n",
    "\n",
    "            \n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            # Forward pass\n",
    "            outputs = net(features.double())\n",
    "#             print('output device:',outputs.get_device())\n",
    "            outputs, predicted_labels = torch.transpose(outputs, 0, 1)\n",
    "            output_loss = criterion(outputs, targets.double())\n",
    "            label_loss = label_criterion(predicted_labels, labels.double())\n",
    "            loss = (alpha * output_loss + beta * label_loss)/2\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate and print the average loss for the epoch\n",
    "        average_loss = epoch_loss / len(train_dataloader)\n",
    "        if (((epoch+1)%10)==0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        out_loss = 0.0\n",
    "        total_f1 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                features, targets, labels = batch['features'], batch['target'], batch['label']\n",
    "\n",
    "                features=features.squeeze(1)\n",
    "                targets=targets.squeeze(1)\n",
    "                labels=labels.squeeze(1)\n",
    "                \n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = net(features.double())\n",
    "                outputs, predicted_labels = torch.transpose(outputs, 0, 1)\n",
    "                output_loss = criterion(outputs, targets.double())\n",
    "                label_loss = label_criterion(predicted_labels,labels.double())\n",
    "                loss=(alpha * output_loss + beta * label_loss)/2\n",
    "                out_loss += output_loss.item()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate F1 score\n",
    "                outputs_sigmoid = torch.sigmoid(predicted_labels)\n",
    "                outputs_np = outputs_sigmoid.cpu().numpy().flatten()\n",
    "                targets_np = labels.cpu().numpy().flatten()\n",
    "                outputs_bin = (outputs_np > 0.5).astype(int)  # Binarize the outputs\n",
    "                # print(outputs_bin, targets_np)\n",
    "                # targets_bin = targets_np.astype(int)\n",
    "                f1 = f1_score(targets_np, outputs_bin, average='macro')\n",
    "                total_f1 += f1\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        out_loss /= len(val_dataloader)\n",
    "        average_f1 = total_f1 / len(val_dataloader)\n",
    "        if (((epoch+1) %10)==0):\n",
    "            print(f'Validation Loss: {output_loss:.4f}, Validation F1 Score: {average_f1:.4f}, Label loss:{label_loss:.4f}, {alpha:.3f},{beta:.3f}')\n",
    "\n",
    "        save = Path(f'{data_path[:-1]}/ablation/')\n",
    "\n",
    "        if not save.exists():\n",
    "            save.mkdir(parents=True)\n",
    "            \n",
    "        torch.save(net.state_dict(),f'{data_path[:-1]}/ablation/model_{depth}.pth')\n",
    "\n",
    "        if scdulr=='Platue':\n",
    "            # scheduler.step(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "        if scdulr=='Cosine':\n",
    "            scheduler.step()\n",
    "        \n",
    "\n",
    "    # Final evaluation on test data\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    total_f1 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            features, targets, labels = batch['features'], batch['target'], batch['label']\n",
    "\n",
    "            features=features.squeeze(1)\n",
    "            targets=targets.squeeze(1)\n",
    "            labels=labels.squeeze(1)\n",
    "            \n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = net(features.double())\n",
    "            \n",
    "            outputs, predicted_labels = torch.transpose(outputs, 0, 1)\n",
    "            output_loss = criterion(outputs, targets.double())\n",
    "            label_loss = label_criterion(predicted_labels,labels.double())\n",
    "            loss=(output_loss + label_loss *0.5)/2\n",
    "            # loss=(output_loss + label_loss)/2\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate F1 score\n",
    "            outputs_sigmoid = torch.sigmoid(predicted_labels)\n",
    "            outputs_np = outputs_sigmoid.cpu().numpy().flatten()\n",
    "            targets_np = labels.cpu().numpy().flatten()\n",
    "            outputs_bin = (outputs_np > 0.5).astype(int)  # Binarize the outputs\n",
    "            # print(outputs_bin, targets_np)\n",
    "            # targets_bin = targets_np.astype(int)\n",
    "            f1 = f1_score(targets_np, outputs_bin, average='macro')\n",
    "            total_f1 += f1\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    average_f1 = total_f1 / len(test_dataloader)\n",
    "    \n",
    "    val_error.append(val_loss)\n",
    "    tst_error.append(test_loss)\n",
    "    \n",
    "    print(f'Final Test Loss: {test_loss:.4f}, Final Test F1 Score: {average_f1:.4f}, Label loss:{label_loss}')\n",
    "\n",
    "    print(\"Training complete\")\n",
    "\n",
    "    return average_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf6195-3e08-421e-a30b-a3b42b1568e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb51da-4c85-4277-bcf7-428dab4c53a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b37c8f-94e2-4eb0-9cee-005cd5b6ca94",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a08fe681-31d5-4aa1-be71-4c14fb1aed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern1', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern1_1', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern1_2', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern1_3', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern1_4', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern2', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern2_1', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern2_2', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern2_3', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern2_4', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern3', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern3_1', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern3_2', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern3_3', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern3_4', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern4', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern4_1', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern4_2', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern4_3', '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/pattern4_4']\n",
      "\n",
      "\n",
      " Depth value is: 5 Thresh: 97\n",
      "[0.9411764705882353, 0.8750000000000001, 0.9411764705882353, 0.8750000000000001, 0.9090909090909091]\n",
      "0.9082887700534761 0.6206896551724138\n",
      "[0.9090909090909091, 0.8750000000000001, 0.8484848484848485, 0.8750000000000001, 0.8387096774193548]\n",
      "0.8692570869990226 0.6792452830188679\n",
      "[0.9444444444444444, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353, 0.9411764705882353]\n",
      "0.9418300653594771 0.8181818181818181\n",
      "[0.8571428571428571, 0.7878787878787877, 0.75, 0.823529411764706, 0.8484848484848485]\n",
      "0.8134071810542398 0.6792452830188679\n",
      "[0.8831957758665538]\n"
     ]
    }
   ],
   "source": [
    "path=glob.glob('/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/*')\n",
    "\n",
    "patterns = [ i for i in path if i[-3:] !='jpg']\n",
    "patterns = [ i for i in patterns if i[-3:] !='csv']\n",
    "\n",
    "gt_list = [ i for i in path if i[-3:] =='csv']\n",
    "patterns=natsorted(patterns)\n",
    "patterns=patterns[:-1]\n",
    "model_path = '/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/r_top=3.0,r_bottom=0.02'\n",
    "print(patterns)\n",
    "depth=5\n",
    "all_thresh_val,all_thresh_std=[],[]\n",
    "thresh=97\n",
    "\n",
    "model = Net(num_iteration = depth)\n",
    "model = model.double()\n",
    "print('\\n\\n Depth value is:',depth,'Thresh:',thresh)\n",
    "model.load_state_dict(torch.load(f'{model_path}/(7, 11)/dong/model_{depth}.pth',map_location=torch.device('cpu')))\n",
    "model=model.eval()\n",
    "\n",
    "def adc_to_vol(arr):\n",
    "    out=np.array(arr)*(5/1024)\n",
    "    return out\n",
    "\n",
    "\n",
    "f1_pattern,all_pattern_f1,all_pattern_std = [],[],[]\n",
    "f1_before,all_pattern_f1_before,all_pattern_std_before = [],[],[]\n",
    "\n",
    "\n",
    "temp=[]\n",
    "for i in range(len(patterns)):\n",
    "    if((i)%5==0):\n",
    "        a=patterns[i].split('/')\n",
    "        ground_t = np.loadtxt('/Users/shubhamrohal/Documents/GitHub/origami-sensor/figure_7x11/realworld_data2/'+a[-1]+'.csv',delimiter=',')\n",
    "\n",
    "\n",
    "    files=glob.glob(patterns[i]+'/*.csv')\n",
    "\n",
    "    input_arr=np.loadtxt(files[1000],delimiter=',')\n",
    "\n",
    "    input_arr=adc_to_vol(input_arr)\n",
    "    input_arr = np.transpose(input_arr)/5.0\n",
    "    feature = torch.unsqueeze(torch.tensor(input_arr, dtype=torch.float32),0) \n",
    "    output = model(feature.double())\n",
    "    input_arr = np.transpose(input_arr)\n",
    "\n",
    "    output, predicted_label = torch.transpose(output, 0, 1)\n",
    "    output = output.squeeze(0)\n",
    "    output = output.double().detach().numpy()\n",
    "\n",
    "    output=np.transpose(output)\n",
    "    output = output*100\n",
    "    output = np.where(output > thresh, 0, 1)\n",
    "\n",
    "    predicted_label = torch.sigmoid(predicted_label)\n",
    "    predicted_label = predicted_label.squeeze(0)\n",
    "    predicted_label = predicted_label.double().detach().numpy()\n",
    "    predicted_label = (predicted_label > 0.5).astype(int)\n",
    "    predicted_label = np.transpose(predicted_label)\n",
    "    orgnl = input_arr\n",
    "    input_arr = np.where(input_arr>0.1,1,0)\n",
    "\n",
    "    test = output*input_arr\n",
    "\n",
    "    flat_test=test.flatten()\n",
    "    flat_gt=ground_t.flatten()\n",
    "    flat_in=input_arr.flatten()\n",
    "\n",
    "    f1_input = f1_score(flat_gt, flat_in, average='binary')\n",
    "    f1_after = f1_score(flat_gt, flat_test, average='binary')\n",
    "\n",
    "    f1_pattern.append(f1_after)\n",
    "    f1_before.append(f1_input)\n",
    "\n",
    "\n",
    "    if((i+1)%5==0):\n",
    "        print(f1_pattern)\n",
    "        print(np.mean(f1_pattern),np.mean(f1_before))\n",
    "\n",
    "        temp.append(np.mean(f1_pattern))\n",
    "\n",
    "        all_pattern_f1.append(np.mean(f1_pattern))\n",
    "        all_pattern_std.append(np.std(f1_pattern))\n",
    "\n",
    "        all_pattern_f1_before.append(np.mean(f1_before))\n",
    "        all_pattern_std_before.append(np.std(f1_before))\n",
    "\n",
    "\n",
    "        f1_pattern=[]\n",
    "        f1_before=[]\n",
    "all_thresh_val.append(np.mean(temp))\n",
    "all_thresh_std.append(np.std(temp))\n",
    "print(all_thresh_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637b4e7-87eb-42ef-bc34-72a1a76dc64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
